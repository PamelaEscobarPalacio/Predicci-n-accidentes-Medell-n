# -*- coding: utf-8 -*-
"""Predicción accidentes Medellín y Clustering

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fbxRb1A4ccKLWk3y3wKOB2lXuDjvExsV
"""

!pip install pyjanitor==0.20
!pip install holidays_co
!pip install datar
!pip install dfply

"""##Lectura de librerias"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import janitor
import matplotlib.pyplot as plt
import seaborn as sb
from google.colab import drive
import numpy as np
from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc
from sklearn.feature_selection import f_classif
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from scipy.stats import chi2_contingency
# %matplotlib inline
import plotnine as p9
import holidays_co
from datar.dplyr import group_by, summarise, n
from datetime import datetime
from sklearn.cluster import KMeans

"""##**Lectura de los datos**"""

datos= pd.read_csv('/content/data_limpia-1.csv', sep = ",", encoding='utf-8')
festivos = pd.read_excel('/content/festivos.xlsx')

datos

"""## Depuración"""

datos.head(3)

datos.tail(3)

festivos.head(4)

festivos['holidays_fecha']=pd.to_datetime(festivos['holidays_fecha'], format='%Y/%m/%d %H:%M:%S')

festivos.info()

datos.shape

"""Limpiamos y estandarizamos los nombres de las columnas del dataframe"""

datos = janitor.clean_names(datos)

datos[datos["ano"] == "2014"]

datos

"""***Contamos con 242.008 datos, despues de haber hecho la limpieza de los mismos en pasos anteriores.***

**Verificando valores faltantes de las variables**
"""

datos.info()

datos.head(5)

"""**Resumen de datos numericos**"""

datos.describe()

"""**Variables descriptivas**

***Tipo accidente***, se ve que la mayoría de datos presentan accidente de tipo Choque con 164.389, luego le sigue Atropello	con 22.948 y le sigue Caida Ocupante con	20.713 datos en este tipo de accidente.
"""

datos.clase_accidente.unique()

(datos
  .groupby("clase_accidente")
  .agg(frequency=("clase_accidente", "count")))

"""***Diseño de la via***, se ve que la mayoría de datos presentan accidentes en el diseño tramo de vía con 168.649 datos, luego se encuentra Interseccion	con 40.868 datos y le sigue Lote o Predio	con 22.307 accidentes en este diseño de via."""

datos.diseno.unique()

(datos
  .groupby("diseno")
  .agg(frequency=("diseno", "count")))

"""***Gravedad aacidente***, se ve que la mayoría de accidentes presentan heridos	con una cantidad de 131.558, luego se encuentra accidentes con Solo daños y una cantidad de	109216, finalmente le sigue accidentes Con muertos con un total de 1.234"""

datos.gravedad_accidente.unique()

(datos
  .groupby("gravedad_accidente")
  .agg(frequency=("gravedad_accidente", "count")))

"""***Años***, la mayoria de accidentes ocurrieron en el año 2016 con 43.456 accidentes, luego 2015 y le sigue 2017. De ultimo tenemos 2020 con 10073 lo cual se puede deber a que la base de datos no abarca todo el año completo y tambien debido a la pandemia hay menos carros en circulación en las calles y por tal motivo menor probabilidad de accidentes."""

datos.ano.unique()

(datos
  .groupby("ano")
  .agg(frequency=("ano", "count"))
  .sort_values(by='frequency', ascending = False))

"""***Mes***, vemos que la mayoria de accidentes ocurren en el mes de agosto con un total de	24.077 accidentes, el siguiente mes con mas accidentes es septiembre con 22.392 y luego julio con una cantidad de	22.346 accidentes"""

datos.mes.unique()

(datos
  .groupby("mes")
  .agg(frequency=("mes", "count")))

"""***Comuna***, estan las comunas 10,  2,  1, 15, 14, 11,  7,  6,  4,  9,  5, 80, 13,  8, 16,  3, 12, 70, 60, 90, 50. La comuna con mayor accidentes es la comuna 10 'La Candelaria' y le sigue la comuna 11 'Laureles Estadio', luego esta la comuna 5 'Castilla' con 23.206 accidentes."""

datos.numcomuna.unique()

(datos
  .groupby("numcomuna")
  .agg(frequency=("numcomuna", "count")))

datos.comuna.unique()

len(datos.comuna.unique())

tabla1 = (datos
  .groupby("comuna")
  .agg(frequency=("comuna", "count"))
  .sort_values(by='frequency', ascending = False))

tabla1[tabla1["frequency"] >=2]

"""***Barrio***, presenta 313 barrios, el que presenta mayor cantidad de accidentes es La Candelaria	con 5.897 accidentes, luego Campo Amor	con 5.135 y finalmente esta el Perpetuo Socorro	con 5.060. Por otro lado está el Corregimiento de San Sebastián de Palmitas, el cual tiene la menor cantidad de accidentes.


"""

datos.barrio.unique()

datos.select_dtypes(include='object').columns

len(datos.barrio.unique())

tabla2 = (datos
  .groupby("barrio")
  .agg(frequency=("barrio", "count"))
  .sort_values(by='frequency', ascending = False))

tabla2[tabla2["frequency"] >= 100]

"""##Tipo de accidente vs cantidad de accidentes por año:
En el siguiente grafico se logra ver la cantidad de accidentes que hay por cada tipo de accidente en los años 2014, 2015, 2016, 2017, 2018, 2019, 2020 respectivamente.
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Crea el gráfico con facet_wrap
grafico = (p9.ggplot(datos, p9.aes(x='clase_accidente', fill='clase_accidente'))
    + p9.geom_bar()
    + p9.facet_wrap('ano', ncol=3)
    + p9.labs(x='Clase de accidentes', y='Cantidad accidentes')
    + p9.theme(strip_text=p9.element_blank())  # Ajusta el ángulo aquí
)
grafico += p9.theme(axis_text_x=p9.element_text(angle=45, hjust=1))
# Dibuja el gráfico
grafico.draw()

"""Cambiamos el formato de ***fecha accidente***"""

datos['fecha_accidente'] = datos['fecha_accidente'].apply(lambda x: x.split(' ')[0])
datos['fecha_accidente'] = pd.to_datetime(datos['fecha_accidente'], format="%d/%m/%Y")

datos['fecha_accidente']

"""##Predicción con Choque

Realizamos predicción con el tipo de accidente ***Choque*** debido a que es el tipo de accidente que más datos presenta, lo cual puede permitir una mejor preddición a los años 2021 y 2022
"""

conteos = (
     datos.groupby(['fecha_accidente', 'clase_accidente'])
     .size()
     .reset_index(name='y')
)

conteos = pd.DataFrame(conteos)
type(conteos)

conteos.info()

conteos['clase_accidente'].unique()

conteos['fecha_accidente']

conteos.info()

conteos=conteos[conteos["clase_accidente"]=="Choque"]
conteos["fecha"] = conteos.fecha_accidente.dt.date
conteos['fecha']=pd.to_datetime(conteos['fecha'], format='%Y/%m/%d')
# conteos = conteos.groupby(["fecha"]).sum().reset_index()

conteos["ano"] = conteos.fecha.dt.year
conteos["dia"] = conteos.fecha.dt.day_name()
conteos["mes"] = conteos.fecha.dt.month
conteos['semana_del_mes'] = conteos['fecha'].apply(lambda d: (d.day-1) // 7 + 1)

conteos

conteos['dia_especial'] = np.where(conteos.loc[:,'fecha'].isin(festivos['holidays_fecha']),1,0)
conteos['quincena'] = np.where(conteos.loc[:,'fecha'].dt.day.isin([15,30,31]),1,0)

"""Revisión de outliers"""

from scipy import stats
import numpy as np

z = np.abs(stats.zscore(conteos["y"]))

conteos["Z_CT"]=z
conteos[conteos["Z_CT"]>1.5]

"""Se sacan los que aparecen por encima de 2"""

conteos=conteos[conteos["Z_CT"]<=1.5]

"""Se crea una variable para convertir las fechas en int Se guarda el dataframe"""

conteos['mes'] = conteos['mes'].astype('str')
conteos['dia_especial'] = conteos['dia_especial'].astype('str')
conteos['semana_del_mes'] = conteos['semana_del_mes'].astype('str')
conteos['quincena'] = conteos['quincena'].astype('str')

"""Se selecciona el conjunto de entrenamiento entre los años 2014 y 2018, y el conjunto de prueba solo con los datos de 2019, debido a que si se evalua con los datos de 2020 no se obtiene una prediccion correcta, ya que los datos presentes en el año 2020 presentan una particularidad y es que fue el año donde inicio la pandemia y habian mayores restricciones para las personas salir. Por tal motivo, incluir el año 2020 podria generar una incorrecta predicción."""

train = conteos[conteos["ano"] <=2018]

test = conteos[conteos["ano"] ==2019]

test

train

train.drop(['ano', 'fecha'], axis = 1, inplace = True)
train.info()

test.drop(['ano', 'fecha'], axis = 1, inplace = True)
test.info()

formula = 'y~ dia+mes+semana_del_mes+dia_especial+quincena'

import statsmodels.formula.api as smf
import statsmodels.api as sm

model = smf.glm(formula = formula, data=train, family=sm.families.Poisson()).fit()

model.summary()

""" ## El criterio de éxito del modelo predictivo será el error cuadrático medio de la predicción, los cuales nos muestran a continuación que el modelo esta bien entrenado para realizar la predicción para accidentes tipo Choque en los periodos de tiempo 2021 y 2022. Recuerda que no se realizó predicción para el año 2020 debido a que fue un año particularmente atipico y el historico de 2014 al 2018 se comportan muy diferentes a este."""

predict_tr = model.predict(train)

mse_train = np.mean(train.y - predict_tr)**2
mse_train

predict = model.predict(test)

mse_test = np.mean(test.y - predict)**2
mse_test

train.head()

"""# Predicción realizada para los años 2021 y 2022"""

from pandas.core import apply
from datetime import datetime, date, timedelta
import holidays_co

inicio = datetime(2021,1,1)
fin    = datetime(2022,12,31)

lista_fechas = [inicio + timedelta(days=d) for d in range((fin - inicio).days + 1)]


validacion = pd.DataFrame(lista_fechas, columns=['fecha'])

validacion["anio"] = validacion.fecha.dt.year
validacion["dia"] = validacion.fecha.dt.day_name()
validacion["mes"] = validacion.fecha.dt.month
validacion['semana_del_mes'] = validacion['fecha'].apply(lambda d: (d.day-1) // 7 + 1)
validacion['quincena'] = np.where(validacion.loc[:,'fecha'].dt.day.isin([15,30,31]),1,0)
validacion['dia_especial'] = np.where(validacion["fecha"].apply(lambda x: holidays_co.is_holiday_date(x)),1,0)

validacion.head()

validacion['mes'] = validacion['mes'].astype('str')
validacion['dia_especial'] = validacion['dia_especial'].astype('str')
validacion['semana_del_mes'] = validacion['semana_del_mes'].astype('str')
validacion['quincena'] = validacion['quincena'].astype('str')

validacion.head()

validacion["prediccion"] = model.predict(validacion)

validacion.head()

validacion['fecha_index'] = validacion['fecha']
validacion['fecha_index2'] = validacion['fecha']

# Set fecha as the index for the DataFrame
validacion.set_index('fecha_index', inplace=True)

# Aggregate by week
weekly_data = validacion.resample('W-MON').agg({
    'prediccion': 'sum'
}).reset_index()

weekly_data.rename(columns={'fecha_index': 'semana'}, inplace=True)

weekly_data

validacion.set_index('fecha_index2', inplace=True)
# Aggregate by month
monthly_data = validacion.resample("MS").agg({
    'prediccion': 'sum'
}).reset_index()

monthly_data.rename(columns={'fecha_index2': 'Mes'}, inplace=True)

print(monthly_data)

# Using plotly.express
import plotly.express as px

fig = px.line(validacion, x='fecha', y="prediccion",
              title="Predicción por cada fecha de cantidad de accidentes tipo choque en Medellín 2021 - 2022")
fig.show()

# Using plotly.express
import plotly.express as px

fig = px.line(weekly_data, x='semana', y="prediccion",
              title="Predicción semanal de cantidad de accidentes tipo Choque en Medellín 2021 - 2022")
fig.show()

# Using plotly.express
import plotly.express as px

fig = px.line(monthly_data, x='Mes', y="prediccion",
              title="Predicción mensual de cantidad de accidentes tipo Choque en Medellín 2021 - 2022")
fig.show()
#?px.line

"""###  Comportamiento de las predicciones 2021 2022"""

from datetime import datetime, date, timedelta
import holidays_co

# La fecha se va a recibir en el formato AAAA/MM/DD
def prediccion(fecha_inicial, fecha_final, df):
  inicio = datetime.strptime(fecha_inicial, '%Y/%m/%d')
  fin = datetime.strptime(fecha_final, '%Y/%m/%d')

  mask = (df['fecha'] >= inicio) & (df['fecha'] <= fin)

  accidentes = df.loc[mask]['prediccion'].sum()

  return accidentes

type(datetime.strptime('2020/01/01', '%Y/%m/%d'))

validacion

"""##Predicción de choques para el rango de tiempo establecido:"""

prediccion('2022/06/18', '2022/06/18', validacion)

"""##Agrupamiento de barrios

Ahora realizamos un nuevo dataframe que solo contiene las siguientes  variables para realizarles clusterización a todos los barrios:
- Cantidad de muertes
- Cantidad de choques
-Cantidad de accidentes segun el diseño de la via
"""

x = datos[['barrio','gravedad_accidente']].value_counts()

x.index

nuevo_datos = pd.DataFrame(x)
nuevo_datos = nuevo_datos.reset_index()

nuevo_datos = nuevo_datos.pivot(index='barrio', columns='gravedad_accidente', values=0).reset_index()
nuevo_datos = nuevo_datos.fillna(0)

nuevo_datos = nuevo_datos[['barrio','Con muertos']]
nuevo_datos

x1 = datos[['barrio','clase_accidente']].value_counts()
x1

nuevo_datos1 = pd.DataFrame(x1)
nuevo_datos1 = nuevo_datos1.reset_index()

nuevo_datos1 = nuevo_datos1.pivot(index='barrio', columns='clase_accidente', values=0).reset_index()
nuevo_datos1 = nuevo_datos1.fillna(0)

nuevo_datos1 = nuevo_datos1[['barrio','Choque']]
nuevo_datos1

diseño = datos[['barrio','diseno']].value_counts()
datos_diseño = pd.DataFrame(diseño)
datos_diseño = diseño.reset_index()
datos_diseño = datos_diseño.pivot(index='barrio', columns='diseno', values=0).reset_index()
datos_diseño = datos_diseño.fillna(0)
datos_diseño

juntos = pd.merge(nuevo_datos,nuevo_datos1,how='inner',on="barrio")
juntos2 = pd.merge(juntos,datos_diseño,how='inner',on="barrio")

"""Matriz de correlación sin dropear"""

corrMatrix=juntos2.drop(['Choque','Ciclo Ruta','Interseccion','Lote o Predio','Tramo de via'],axis=1).corr()
sb.heatmap(corrMatrix, annot=True)
plt.rcParams["figure.figsize"] = (20,20)
plt.show

m = ~(corrMatrix.mask(np.eye(len(corrMatrix), dtype=bool)).abs() > 0.7).any()
raw = corrMatrix.loc[m, m]
raw

import scipy.cluster.hierarchy as shc
from matplotlib import pyplot

pyplot.figure(figsize=(15, 7))
pyplot.title("Dendrograma")
dend = shc.dendrogram(shc.linkage(juntos2.drop(['Choque','Ciclo Ruta','Interseccion','Lote o Predio','Tramo de via','barrio'],axis=1), method='ward'),truncate_mode="lastp")# Dendrograma usando ward como método de linkage.
ax = plt.gca()
bounds = ax.get_xbound()
ax.plot(bounds, [200000, 200000], '--', c='k')
ax.text(bounds[1], 200000, ' Seis grupos', va='center', fontdict={'size': 15})
plt.xlabel("Cantidad de samples <(n)>, índice <i>")
plt.ylabel("Distancias de grupos")
plt.show()

def elbow_curve(data, maxClusters = 15):

  # rango de valores del parámetro a optimizar (cantidad de clusters)
  maxClusters = range(1, maxClusters + 1)
  inertias = []

  # se ejecuta el modelo para el rango de clusters y se guarda la inercia
  # respectiva obtenida para cada valor
  for k in maxClusters:
    kmeanModel = KMeans(n_clusters = k)
    kmeanModel.fit(data)
    inertias.append(kmeanModel.inertia_)

# Grafico de los resultados obtenidos para cada valor del rango
  print("Valores: ",inertias)
  plt.figure(figsize=(10, 8))
  plt.plot(maxClusters, inertias, 'bx-')
  plt.xlabel('k')
  plt.ylabel('Inertia')
  plt.title('The Elbow Method showing the optimal k')
  plt.show()

elbow_curve(np.array(juntos2.drop(['Choque','Ciclo Ruta','Interseccion','Lote o Predio','Tramo de via','barrio'],axis=1)))

from sklearn.metrics import silhouette_samples, silhouette_score
import matplotlib.cm as cm

colors_k_means = ['cyan','purple','orange']
range_n_clusters = list(range(2,15))
X = np.array(juntos2.drop(['Choque','Ciclo Ruta','Interseccion','Lote o Predio','Tramo de via','barrio'],axis=1))

for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(19, 4)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, 1]
    ax1.set_xlim([-0.1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters=n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = plt.cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = plt.cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors, edgecolor='k')

    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                c="white", alpha=1, s=200, edgecolor='k')

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                    s=50, edgecolor='k')

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")

    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

plt.show()

from sklearn.cluster import AgglomerativeClustering
clustering= AgglomerativeClustering(n_clusters=4,linkage="ward")
clustering.fit(juntos2.drop(['Choque','Ciclo Ruta','Interseccion','Lote o Predio','Tramo de via','barrio'],axis=1))
clustering.labels_

final=juntos2
final["cluster"]=clustering.labels_

pd.set_option('display.max_rows', 99999)

final

final = final.iloc[7:].reset_index()
final.drop(['index'], axis=1, inplace=True)
final

import plotly.express as px
import plotly.graph_objects as go

fig = px.scatter_3d(final, x="Glorieta", y="Paso Elevado", z="Con muertos",color="cluster",size_max=0.0001)
fig.show()

"""##**Con dropeo**"""

corrMatrix=juntos2.drop(['barrio'],axis=1).corr()
sb.heatmap(corrMatrix, annot=True)
plt.rcParams["figure.figsize"] = (20,20)
plt.show

import scipy.cluster.hierarchy as shc
from matplotlib import pyplot

pyplot.figure(figsize=(15, 7))
pyplot.title("Dendrograma")
dend = shc.dendrogram(shc.linkage(juntos2.drop(['barrio'],axis=1), method='ward'),truncate_mode="lastp")# Dendrograma usando ward como método de linkage.
ax = plt.gca()
bounds = ax.get_xbound()
ax.plot(bounds, [200000, 200000], '--', c='k')
ax.text(bounds[1], 200000, ' Seis grupos', va='center', fontdict={'size': 15})
plt.xlabel("Cantidad de samples <(n)>, índice <i>")
plt.ylabel("Distancias de grupos")
plt.show()

from sklearn.cluster import KMeans
def elbow_curve(data, maxClusters = 15):

  # rango de valores del parámetro a optimizar (cantidad de clusters)
  maxClusters = range(1, maxClusters + 1)
  inertias = []

  # se ejecuta el modelo para el rango de clusters y se guarda la inercia
  # respectiva obtenida para cada valor
  for k in maxClusters:
    kmeanModel = KMeans(n_clusters = k)
    kmeanModel.fit(data)
    inertias.append(kmeanModel.inertia_)

# Grafico de los resultados obtenidos para cada valor del rango
  print("Valores: ",inertias)
  plt.figure(figsize=(10, 8))
  plt.plot(maxClusters, inertias, 'bx-')
  plt.xlabel('k')
  plt.ylabel('Inertia')
  plt.title('The Elbow Method showing the optimal k')
  plt.show()

elbow_curve(np.array(juntos2.drop(['barrio'],axis=1)))

from sklearn.metrics import silhouette_samples, silhouette_score
import matplotlib.cm as cm

colors_k_means = ['cyan','purple','orange']
range_n_clusters = list(range(2,15))
X = np.array(juntos2.drop(['barrio'],axis=1))

for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(19, 4)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, 1]
    ax1.set_xlim([-0.1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters=n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = plt.cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = plt.cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors, edgecolor='k')

    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                c="white", alpha=1, s=200, edgecolor='k')

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                    s=50, edgecolor='k')

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")

    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

plt.show()

from sklearn.cluster import AgglomerativeClustering
clustering= AgglomerativeClustering(n_clusters=6,linkage="ward")
clustering.fit(juntos2.drop(['barrio'],axis=1))
clustering.labels_

final=juntos2
final["cluster"]=clustering.labels_

final

"""##Guardamos los datos finales con cada cluster"""

final.to_csv('final_clusters.csv')

import plotly.express as px
import plotly.graph_objects as go

fig = px.scatter_3d(final, x="Glorieta", y="Con muertos", z="Paso Elevado",color="cluster",labels='barrio',size_max=0.0001)
fig.show()

final.describe()

result=final.groupby('cluster').mean().reset_index()
result

"""##***Descripción de cada cluster***

***Cluster 0:*** Es el grupo de barrios con menos accidentes viales.


*Caracteristicas:*

- Número de muertos: 1
- Número de choques: 133
- El tipo de via que presentó mayor cantidad de accidentes es en Tramo de via
- El segundo tipo de via que presentó mayor cantidad de accidentes es en intersecciones y en intersecciones
- Los tipos de via con menos accidentes son ciclo ruta, pasos elevado, inferior y a nivel, puentes, ponton y via peatonal.
"""

result_cluster_0 = result[result['cluster'] == 0]
result_cluster_0

"""***Cluster 1:*** Es el tercer grupo de barrios con más accidentes viales.




*Caracteristicas:*

- Número de muertos: 11
- Número de choques: 1,749
- El tipo de via que presentó mayor cantidad de accidentes es en Tramo de via con 1.563 accidentes
- El segundo tipo de via que presentó mayor cantidad de accidentes es en intersecciones y en Lote o Predio
- Los tipos de via con menos accidentes son via peatonal, tunel y ponton
"""

result_cluster_1 = result[result['cluster'] == 1]
result_cluster_1

"""***Cluster 2:*** Es el tercer grupo de barrios con menos accidentes viales.




*Caracteristicas:*

- Número de muertos: 7
- Número de choques: 881
- El tipo de via que presentó mayor cantidad de accidentes es en Tramo de via
- El segundo tipo de via que presentó mayor cantidad de accidentes es en intersecciones y luego Lote o Predio
- Los tipos de via con menos accidentes son via peatonal, paso a nivel, tunel y pontón
"""

result_cluster_2 = result[result['cluster'] == 2]
result_cluster_2

"""***Cluster 3:*** Es el grupo de barrios con mayor cantidad de accidentes viales.

*Caracteristicas:*

- Número de muertos: 21
- Número de choques: 3,774
- El tipo de via que presentó mayor cantidad de accidentes es en Tramo de via con 3,617
- El segundo tipo de via que presentó mayor cantidad de accidentes es en interseccione
- Los tipos de via con menos accidentes son tunel y pontó
"""

result_cluster_3 = result[result['cluster'] == 3]
result_cluster_3

"""***Cluster 4:*** Es el segundo grupo de barrios con mayor cantidad de accidentes viales.

*Caracteristicas:*

- Número de muertos:
- Número de choques: 2,655
- El tipo de via que presentó mayor cantidad de accidentes es en Tramo de v
- Los tipos de via con menos accidentes son via peatonal y pontón
"""

result_cluster_4 = result[result['cluster'] == 4]
result_cluster_4

"""***Cluster 5:*** Es el segundo grupo de barrios con menos cantidad de accidentes viales.

*Caracteristicas:*

- Número de muertos: 4
- Número de choques: 511
- El tipo de via que presentó mayor cantidad de accidentes es en Tramo de via e intersecciones
- Los tipos de via con menos accidentes son Paso Elevado, Paso Inferior, Paso a Nivel,	Pontón,	Puente,	Tramo de via Tunel y	Via peatonal.
"""

result_cluster_5 = result[result['cluster'] == 5]
result_cluster_5