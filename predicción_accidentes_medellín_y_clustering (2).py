# -*- coding: utf-8 -*-
"""Predicción accidentes Medellín y Clustering

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ehv7vbgwXsVOmy9GUtLC0i1VTf8QLPnA
"""

!pip install pyjanitor==0.20
!pip install holidays_co
!pip install datar
!pip install dfply

"""##Lectura de librerias"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import janitor
import matplotlib.pyplot as plt
import seaborn as sb
from google.colab import drive
import numpy as np
from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc
from sklearn.feature_selection import f_classif
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from scipy.stats import chi2_contingency
# %matplotlib inline
import plotnine as p9
import holidays_co
from datar.dplyr import group_by, summarise, n
from datetime import datetime
from sklearn.cluster import KMeans
from scipy import stats
import statsmodels.formula.api as smf
import statsmodels.api as sm
from pandas.core import apply
from datetime import datetime, date, timedelta
from sklearn.cluster import AgglomerativeClustering
import matplotlib.cm as cm
from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

"""##**Lectura de los datos**

Se utilizaron datos con las siguientes caracteristicas:
- Navidad: incluye los dias 24 y 25 de diciembre
- dia de padres y madres: incluye dia de madres, dia de padres
- Festivos: cuenta con los dias festivos celebrados en colombia
-Fines de semana: incluye los sabados y domingos
- Halloween: dia de disfraces en Colombia el 31 de octubre
"""

datos= pd.read_csv('/content/data_limpia-1.csv', sep = ",", encoding='utf-8')
festivos = pd.read_excel('/content/festivos.xlsx')
navidad=pd.read_excel('/content/christmas.xlsx')
padres_madres= pd.read_excel('/content/padres_madres.xlsx')
halloween= pd.read_excel('/content/halloween.xlsx')

datos

"""## Depuración"""

datos.head(3)

datos.tail(3)

festivos.head(4)

navidad.head(4)

festivos['holidays_fecha']=pd.to_datetime(festivos['holidays_fecha'], format='%Y/%m/%d %H:%M:%S')

navidad['christmas_fecha']=pd.to_datetime(navidad['christmas_fecha'], format='%Y/%m/%d %H:%M:%S')

padres_madres['padres_madres']=pd.to_datetime(padres_madres['fathers day_mothers day'], format='%Y/%m/%d %H:%M:%S')

halloween['halloween']=pd.to_datetime(halloween['Halloween'], format='%Y/%m/%d %H:%M:%S')

festivos.info()

navidad.info()

datos.shape

"""Limpiamos y estandarizamos los nombres de las columnas del dataframe"""

datos = janitor.clean_names(datos)

datos[datos["ano"] == "2014"]

datos

"""***Contamos con 242.008 datos, despues de haber hecho la limpieza de los mismos en pasos anteriores.***

**Verificando valores faltantes de las variables**
"""

datos.info()

datos.head(5)

"""**Resumen de datos numericos**"""

datos.describe()

"""**Variables descriptivas**

***Tipo accidente***, se ve que la mayoría de datos presentan accidente de tipo Choque con 164.389, luego le sigue Atropello	con 22.948 y le sigue Caida Ocupante con	20.713 datos en este tipo de accidente.
"""

datos.clase_accidente.unique()

(datos
  .groupby("clase_accidente")
  .agg(frequency=("clase_accidente", "count")))

"""***Diseño de la via***, se ve que la mayoría de datos presentan accidentes en el diseño tramo de vía con 168.649 datos, luego se encuentra Interseccion	con 40.868 datos y le sigue Lote o Predio	con 22.307 accidentes en este diseño de via."""

datos.diseno.unique()

(datos
  .groupby("diseno")
  .agg(frequency=("diseno", "count")))

"""***Gravedad aacidente***, se ve que la mayoría de accidentes presentan heridos	con una cantidad de 131.558, luego se encuentra accidentes con Solo daños y una cantidad de	109216, finalmente le sigue accidentes Con muertos con un total de 1.234"""

datos.gravedad_accidente.unique()

(datos
  .groupby("gravedad_accidente")
  .agg(frequency=("gravedad_accidente", "count")))

"""***Años***, la mayoria de accidentes ocurrieron en el año 2016 con 43.456 accidentes, luego 2015 y le sigue 2017. De ultimo tenemos 2020 con 10073 lo cual se puede deber a que la base de datos no abarca todo el año completo y tambien debido a la pandemia hay menos carros en circulación en las calles y por tal motivo menor probabilidad de accidentes."""

datos.ano.unique()

(datos
  .groupby("ano")
  .agg(frequency=("ano", "count"))
  .sort_values(by='frequency', ascending = False))

"""***Mes***, vemos que la mayoria de accidentes ocurren en el mes de agosto con un total de	24.077 accidentes, el siguiente mes con mas accidentes es septiembre con 22.392 y luego julio con una cantidad de	22.346 accidentes"""

datos.mes.unique()

(datos
  .groupby("mes")
  .agg(frequency=("mes", "count")))

"""***Comuna***, estan las comunas 10,  2,  1, 15, 14, 11,  7,  6,  4,  9,  5, 80, 13,  8, 16,  3, 12, 70, 60, 90, 50. La comuna con mayor accidentes es la comuna 10 'La Candelaria' y le sigue la comuna 11 'Laureles Estadio', luego esta la comuna 5 'Castilla' con 23.206 accidentes."""

datos.numcomuna.unique()

(datos
  .groupby("numcomuna")
  .agg(frequency=("numcomuna", "count")))

datos.comuna.unique()

len(datos.comuna.unique())

tabla1 = (datos
  .groupby("comuna")
  .agg(frequency=("comuna", "count"))
  .sort_values(by='frequency', ascending = False))

tabla1[tabla1["frequency"] >=2]

"""***Barrio***, presenta 313 barrios, el que presenta mayor cantidad de accidentes es La Candelaria	con 5.897 accidentes, luego Campo Amor	con 5.135 y finalmente esta el Perpetuo Socorro	con 5.060. Por otro lado está el Corregimiento de San Sebastián de Palmitas, el cual tiene la menor cantidad de accidentes.


"""

datos.barrio.unique()

datos.select_dtypes(include='object').columns

len(datos.barrio.unique())

tabla2 = (datos
  .groupby("barrio")
  .agg(frequency=("barrio", "count"))
  .sort_values(by='frequency', ascending = False))

tabla2[tabla2["frequency"] >= 100]

"""##Tipo de accidente vs cantidad de accidentes por año:
En el siguiente grafico se logra ver la cantidad de accidentes que hay por cada tipo de accidente en los años 2014, 2015, 2016, 2017, 2018, 2019, 2020 respectivamente.
"""

import plotnine as p9

# Crea una lista de títulos personalizados para las columnas
column_titles = ["2014", "2015", "2016", "2017", "2018", "2019", "2020"]

# Crea el gráfico con facet_wrap y asigna títulos personalizados a las columnas
grafico = (p9.ggplot(datos, p9.aes(x='clase_accidente', fill='clase_accidente'))
    + p9.geom_bar()
    + p9.facet_wrap('ano', ncol=3, labeller=p9.labeller(ano=lambda label: column_titles[int(label) - 2014]))
    + p9.labs(x='Clase de accidentes', y='Cantidad accidentes')
    + p9.theme(strip_text=p9.element_text(angle=0))  # Ajusta el ángulo aquí
)
grafico += p9.theme(axis_text_x=p9.element_text(angle=45, hjust=1))

# Dibuja el gráfico
grafico.draw()

"""##Analisis si se realiza la predicción incluyendo las fechas de pandemia

De la grafica anterior, que presenta la cantidad de accidentes según el tipo de accidentes de cada año, es notable la gran diferencia que existe según la cantidad de accidentes que ocurrieron en el 2020 con respecto a los demás años anteriores. Esto clarifica la idea de que al incluir el año 2020 en algun modelo de predicción, este no ayudaria a entender muy bien el comportamiento historico, por ende no seria muy util para la predicción.
Incluir el año 2020 en el modelo de predicción de accidentes en Medellín podría resultar contraproducente debido a las circunstancias excepcionales que caracterizaron ese periodo. El año 2020 estuvo marcado por la pandemia de COVID-19, que llevó a medidas de confinamiento, restricciones de movilidad y cambios significativos en los patrones de comportamiento de la población. Estos factores extraordinarios afectaron drásticamente la cantidad y la naturaleza de los accidentes de tráfico, creando una brecha significativa en la información histórica sobre la cual se basan los modelos predictivos. La anomalía de ese año podría distorsionar la capacidad de los modelos para anticipar de manera precisa los riesgos de seguridad vial en condiciones normales, haciendo que la inclusión de datos del 2020 en estos modelos carezca de utilidad para proyectar de manera efectiva los patrones futuros de accidentes en Medellín.

Cambiamos el formato de ***fecha accidente***
"""

datos['fecha_accidente'] = datos['fecha_accidente'].apply(lambda x: x.split(' ')[0])
datos['fecha_accidente'] = pd.to_datetime(datos['fecha_accidente'], format="%d/%m/%Y")

datos['fecha_accidente']

"""##Predicción con Choque

Realizamos predicción con el tipo de accidente ***Choque*** debido a que es el tipo de accidente que más datos presenta, lo cual puede permitir una mejor preddición a los años 2021 y 2022
"""

conteos = (
     datos.groupby(['fecha_accidente', 'clase_accidente'])
     .size()
     .reset_index(name='y')
)

conteos = pd.DataFrame(conteos)
type(conteos)

conteos.info()

conteos['clase_accidente'].unique()

conteos['fecha_accidente']

conteos.info()

conteos=conteos[conteos["clase_accidente"]=="Choque"]
conteos["fecha"] = conteos.fecha_accidente.dt.date
conteos['fecha']=pd.to_datetime(conteos['fecha'], format='%Y/%m/%d')

conteos["ano"] = conteos.fecha.dt.year
conteos["dia"] = conteos.fecha.dt.day_name()
conteos["mes"] = conteos.fecha.dt.month
conteos['semana_del_mes'] = conteos['fecha'].apply(lambda d: (d.day-1) // 7 + 1)

conteos

conteos['dia_especial'] = np.where(conteos.loc[:,'fecha'].isin(festivos['holidays_fecha']),1,0)
conteos['quincena'] = np.where(conteos.loc[:,'fecha'].dt.day.isin([15,30,31]),1,0)
conteos['navidad'] = np.where(conteos.loc[:,'fecha'].isin(navidad['christmas_fecha']),1,0)
conteos["fin_de_semana"] =np.where((conteos['fecha'].dt.dayofweek >= 5), 1, 0)
conteos["padres_madres"] =np.where(conteos.loc[:,'fecha'].isin(padres_madres['fathers day_mothers day']),1,0)
conteos['halloween'] = np.where(conteos.loc[:,'fecha'].isin(halloween['halloween']),1,0)

"""Revisión de outliers"""

z = np.abs(stats.zscore(conteos["y"]))

conteos["Z_CT"]=z
conteos[conteos["Z_CT"]>1.5]

"""Se sacan los que aparecen por encima de 1.5"""

conteos=conteos[conteos["Z_CT"]<=1.5]

"""Se crea una variable para convertir las fechas en int a str y  se guarda el dataframe"""

conteos['mes'] = conteos['mes'].astype('str')
conteos['dia_especial'] = conteos['dia_especial'].astype('str')
conteos['semana_del_mes'] = conteos['semana_del_mes'].astype('str')
conteos['quincena'] = conteos['quincena'].astype('str')
conteos['navidad'] = conteos['navidad'].astype('str')
conteos['fin_de_semana'] = conteos['fin_de_semana'].astype('str')
conteos['padres_madres'] = conteos['padres_madres'].astype('str')
conteos['halloween'] = conteos['halloween'].astype('str')

"""##Predicción de choques para el rango de tiempo establecido:

Se selecciona el conjunto de entrenamiento entre los años 2014 y 2019, y el conjunto de prueba solo con los datos de 2019, debido a que si se evalua con los datos de 2020 no se obtiene una prediccion correcta, ya que los datos presentes en el año 2020 presentan una particularidad y es que fue el año donde inicio la pandemia y habian mayores restricciones para las personas salir. Por tal motivo, incluir el año 2020 podria generar una incorrecta predicción.
"""

train = conteos[conteos["ano"] <=2018]

test = conteos[conteos["ano"] ==2019]

test

train

train.drop(['ano', 'fecha'], axis = 1, inplace = True)
train.info()

test.drop(['ano', 'fecha'], axis = 1, inplace = True)
test.info()

formula = 'y~halloween+fin_de_semana+navidad+dia_especial+quincena'

model = smf.glm(formula = formula, data=train, family=sm.families.Poisson()).fit()

"""Vemos que la variables de dia de padres y de madres, no es una variable significativa para el modelo, por lo tanto no se tiene en cuenta en el modelo."""

model.summary()

""" ## El criterio de éxito del modelo predictivo será el error cuadrático medio de la predicción, los cuales nos muestran a continuación que el modelo esta bien entrenado para realizar la predicción para accidentes tipo Choque en los periodos de tiempo 2021 y 2022. Recuerda que no se realizó predicción para el año 2020 debido a que fue un año particularmente atipico y el historico de 2014 al 2019 se comportan muy diferentes a este."""

predict_tr = model.predict(train)
predict_tes = model.predict(test)

mse_train = mean_squared_error(train.y, predict_tr)
mse_train

mse_test = mean_squared_error(test.y, predict_tes)
mse_test

variacion=abs((mse_train-mse_test)/mse_train)
variacion

train.head()

"""# Predicción realizada para los años 2021 y 2024"""

inicio = datetime(2021,1,1)
fin    = datetime(2024,12,31)

lista_fechas = [inicio + timedelta(days=d) for d in range((fin - inicio).days + 1)]


validacion = pd.DataFrame(lista_fechas, columns=['fecha'])

validacion["anio"] = validacion.fecha.dt.year
validacion["dia"] = validacion.fecha.dt.day_name()
validacion["mes"] = validacion.fecha.dt.month
validacion['semana_del_mes'] = validacion['fecha'].apply(lambda d: (d.day-1) // 7 + 1)
validacion['quincena'] = np.where(validacion.loc[:,'fecha'].dt.day.isin([15,30,31]),1,0)
validacion['dia_especial'] = np.where(validacion["fecha"].apply(lambda x: holidays_co.is_holiday_date(x)),1,0)
validacion['navidad'] = np.where(validacion.loc[:,'fecha'].isin(navidad['christmas_fecha']),1,0)
validacion["fin_de_semana"] =np.where((validacion['fecha'].dt.dayofweek >= 5), 1, 0)
validacion["halloween"] =np.where(validacion.loc[:,'fecha'].isin(halloween['halloween']),1,0)

validacion.head()

validacion['mes'] = validacion['mes'].astype('str')
validacion['dia_especial'] = validacion['dia_especial'].astype('str')
validacion['semana_del_mes'] = validacion['semana_del_mes'].astype('str')
validacion['quincena'] = validacion['quincena'].astype('str')
validacion['navidad'] = validacion['navidad'].astype('str')
validacion['fin_de_semana'] = validacion['fin_de_semana'].astype('str')
validacion['halloween'] = validacion['halloween'].astype('str')

validacion.head()

validacion["prediccion"] = model.predict(validacion)

validacion.head()

validacion['fecha_index'] = validacion['fecha']
validacion['fecha_index2'] = validacion['fecha']

# Establecer la fecha como índice del DataFrame
validacion.set_index('fecha_index', inplace=True)

# Agregado por semana
weekly_data = validacion.resample('W-MON').agg({
    'prediccion': 'sum'
}).reset_index()

weekly_data.rename(columns={'fecha_index': 'semana'}, inplace=True)

weekly_data

validacion.set_index('fecha_index2', inplace=True)
# Agregado por mes
monthly_data = validacion.resample("MS").agg({
    'prediccion': 'sum'
}).reset_index()

monthly_data.rename(columns={'fecha_index2': 'Mes'}, inplace=True)

print(monthly_data)

import plotly.express as px

fig = px.line(validacion, x='fecha', y="prediccion",
              title="Predicción por cada fecha de cantidad de accidentes tipo choque en Medellín 2021 - 2024",
              line_shape='linear', color_discrete_sequence=['purple'])
fig.show()

# Usando plotly.express
import plotly.express as px

fig = px.line(weekly_data, x='semana', y="prediccion",
              title="Predicción semanal de cantidad de accidentes tipo Choque en Medellín 2021 - 2024",
              line_shape='linear', color_discrete_sequence=['purple'])
fig.show()

# Usando plotly.express
import plotly.express as px

fig = px.line(monthly_data, x='Mes', y="prediccion",
              title="Predicción mensual de cantidad de accidentes tipo Choque en Medellín 2021 - 2024",
              line_shape='linear', color_discrete_sequence=['purple'])
fig.show()

"""###  Comportamiento de las predicciones 2021 2024"""

from datetime import datetime, date, timedelta
import holidays_co

# La fecha se va a recibir en el formato AAAA/MM/DD
def prediccion(fecha_inicial, fecha_final, df):
  inicio = datetime.strptime(fecha_inicial, '%Y/%m/%d')
  fin = datetime.strptime(fecha_final, '%Y/%m/%d')

  mask = (df['fecha'] >= inicio) & (df['fecha'] <= fin)

  accidentes = df.loc[mask]['prediccion'].sum()

  return accidentes

type(datetime.strptime('2020/01/01', '%Y/%m/%d'))

validacion.to_csv('validacion.csv')

"""##Predicción de choques para el rango de tiempo establecido:"""

prediccion('2022/06/06', '2022/06/06', validacion)

"""##Agrupamiento de barrios

Ahora realizamos un nuevo dataframe que solo contiene las siguientes  variables para realizarles clusterización a todos los barrios:
- Cantidad de muertes
- Cantidad de choques
-Cantidad de accidentes segun el diseño de la via
"""

x = datos[['barrio','gravedad_accidente']].value_counts()

x.index

nuevo_datos = pd.DataFrame(x)
nuevo_datos = nuevo_datos.reset_index()

nuevo_datos = nuevo_datos.pivot(index='barrio', columns='gravedad_accidente', values=0).reset_index()
nuevo_datos = nuevo_datos.fillna(0)

nuevo_datos = nuevo_datos[['barrio','Con muertos']]
nuevo_datos

x1 = datos[['barrio','clase_accidente']].value_counts()
x1

nuevo_datos1 = pd.DataFrame(x1)
nuevo_datos1 = nuevo_datos1.reset_index()

nuevo_datos1 = nuevo_datos1.pivot(index='barrio', columns='clase_accidente', values=0).reset_index()
nuevo_datos1 = nuevo_datos1.fillna(0)

nuevo_datos1 = nuevo_datos1[['barrio','Choque']]
nuevo_datos1

diseño = datos[['barrio','diseno']].value_counts()
datos_diseño = pd.DataFrame(diseño)
datos_diseño = diseño.reset_index()
datos_diseño = datos_diseño.pivot(index='barrio', columns='diseno', values=0).reset_index()
datos_diseño = datos_diseño.fillna(0)
datos_diseño

juntos = pd.merge(nuevo_datos,nuevo_datos1,how='inner',on="barrio")
juntos2 = pd.merge(juntos,datos_diseño,how='inner',on="barrio")

"""Matriz de correlación sin dropear"""

corrMatrix=juntos2.drop(['Choque','Ciclo Ruta','Interseccion','Lote o Predio','Tramo de via'],axis=1).corr()
sb.heatmap(corrMatrix, annot=True)
plt.rcParams["figure.figsize"] = (20,20)
plt.show

m = ~(corrMatrix.mask(np.eye(len(corrMatrix), dtype=bool)).abs() > 0.7).any()
raw = corrMatrix.loc[m, m]
raw

colors_k_means = ['cyan','purple','orange']
range_n_clusters = list(range(2,15))
X = np.array(juntos2.drop(['Choque','Ciclo Ruta','Interseccion','Lote o Predio','Tramo de via','barrio'],axis=1))

for n_clusters in range_n_clusters:
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(19, 4)

    # La primera subtrama es la trama de la silueta.
     # El coeficiente de silueta puede variar entre -1, 1 pero en este ejemplo todos
     # se encuentra dentro de [-0.1, 1]
    ax1.set_xlim([-0.1, 1])
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    # Inicializa el clusterer con el valor n_clusters y un generador aleatorio
     # semilla de 10 para reproducibilidad.
    clusterer = KMeans(n_clusters=n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X)

    # Silhouette_score proporciona el valor promedio de todas las muestras.
     # Esto da una perspectiva de la densidad y separación de lo formado.
     # grupos
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("Para n_clusters =", n_clusters,
          "El puntaje promedio de silueta es:", silhouette_avg)

    # Calcular las puntuaciones de silueta para cada muestra.
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Agregar las puntuaciones de silueta para las muestras que pertenecen a
         # agrupar i y ordenarlos
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = plt.cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        y_lower = y_upper + 10

    ax1.set_title("El gráfico de silueta de los distintos grupos..")
    ax1.set_xlabel("Los valores del coeficiente de silueta.")
    ax1.set_ylabel("Etiqueta de grupo")

    # La línea vertical para la puntuación de silueta promedio de todos los valores
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # Segundo gráfico que muestra los grupos reales formados
    colors = plt.cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors, edgecolor='k')

    # Etiquetar los grupos
    centers = clusterer.cluster_centers_
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                c="white", alpha=1, s=200, edgecolor='k')

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                    s=50, edgecolor='k')

    ax2.set_title("La visualización de los datos agrupados.")
    ax2.set_xlabel("Espacio destacado para la primera función.")
    ax2.set_ylabel("Espacio destacado para la segunda función.")

    plt.suptitle(("Análisis de silueta para agrupación de KMeans en datos de muestra"
                  "con n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

plt.show()

clustering= AgglomerativeClustering(n_clusters=4,linkage="ward")
clustering.fit(juntos2.drop(['Choque','Ciclo Ruta','Interseccion','Lote o Predio','Tramo de via','barrio'],axis=1))
clustering.labels_

final=juntos2
final["cluster"]=clustering.labels_
pd.set_option('display.max_rows', 99999)
final

final = final.iloc[7:].reset_index()
final.drop(['index'], axis=1, inplace=True)
final

"""##**Con dropeo**"""

corrMatrix=juntos2.drop(['barrio'],axis=1).corr()
sb.heatmap(corrMatrix, annot=True)
plt.rcParams["figure.figsize"] = (20,20)
plt.show

colors_k_means = ['cyan','purple','orange']
range_n_clusters = list(range(2,15))
X = np.array(juntos2.drop(['barrio'],axis=1))

for n_clusters in range_n_clusters:
    # Crea una subtrama con 1 fila y 2 columnas
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(19, 4)

    # La primera subtrama es la trama de la silueta.
    # El coeficiente de silueta puede variar entre -1, 1 pero en este ejemplo todos
    # se encuentra dentro de [-0.1, 1]
    ax1.set_xlim([-0.1, 1])
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    # Inicializa el clusterer con el valor n_clusters y un generador aleatorio
    # semilla de 10 para reproducibilidad.
    clusterer = KMeans(n_clusters=n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X)

    # Silhouette_score proporciona el valor promedio de todas las muestras.
    # Esto da una perspectiva de la densidad y separación de lo formado.
    # grupos
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("Para n_clusters =", n_clusters,
          "El puntaje promedio de silueta es :", silhouette_avg)

    # Calcular las puntuaciones de silueta para cada muestra.
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Agregar las puntuaciones de silueta para las muestras que pertenecen a
         # agrupar i y ordenarlos
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = plt.cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
        y_lower = y_upper + 10

    ax1.set_title("El gráfico de silueta de los distintos grupos.")
    ax1.set_xlabel("Los valores del coeficiente de silueta.")
    ax1.set_ylabel("Etiqueta de grupo")
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")
    ax1.set_yticks([])
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # Segundo gráfico que muestra los grupos reales formados
    colors = plt.cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors, edgecolor='k')

    # Etiquetar los grupos
    centers = clusterer.cluster_centers_
    # Dibuja círculos blancos en los centros de los grupos.
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                c="white", alpha=1, s=200, edgecolor='k')

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                    s=50, edgecolor='k')

    ax2.set_title("La visualización de los datos agrupados.")
    ax2.set_xlabel("Espacio destacado para la primera función.")
    ax2.set_ylabel("Espacio destacado para la segunda función.")

    plt.suptitle(("Análisis de silueta para agrupación de KMeans en datos de muestra"
                  "con n_clusters= %d" % n_clusters),
                 fontsize=14, fontweight='bold')

plt.show()

clustering= AgglomerativeClustering(n_clusters=6,linkage="ward")
clustering.fit(juntos2.drop(['barrio'],axis=1))
clustering.labels_

final=juntos2
final["cluster"]=clustering.labels_

final

"""##Guardamos los datos finales con cada cluster"""

final.to_csv('final_clusters.csv')

final.describe()

result=final.groupby('cluster').mean().reset_index()
result

"""##***Descripción de cada cluster***

***Cluster 0:*** Es el grupo de barrios con menos accidentes viales.


*Caracteristicas:*

- Número de muertos: 1
- Número de choques: 133
- El tipo de via que presentó mayor cantidad de accidentes es en Tramo de via
- El segundo tipo de via que presentó mayor cantidad de accidentes es en intersecciones y en tramo de via
- Los tipos de via con menos accidentes son ciclo ruta, pasos elevado, inferior y a nivel, puentes, ponton y via peatonal.
"""

result_cluster_0 = result[result['cluster'] == 0]
result_cluster_0

"""***Cluster 1:*** Es el tercer grupo de barrios con más accidentes viales.




*Caracteristicas:*

- Número de muertos: 11
- Número de choques: 1,749
- El tipo de via que presentó mayor cantidad de accidentes es en Tramo de via con 1.563 accidentes
- El segundo tipo de via que presentó mayor cantidad de accidentes es en intersecciones y en Lote o Predio
- Los tipos de via con menos accidentes son via peatonal, tunel y ponton
"""

result_cluster_1 = result[result['cluster'] == 1]
result_cluster_1

"""***Cluster 2:*** Es el tercer grupo de barrios con menos accidentes viales.




*Caracteristicas:*

- Número de muertos: 7
- Número de choques: 881
- El tipo de via que presentó mayor cantidad de accidentes es en Tramo de via
- El segundo tipo de via que presentó mayor cantidad de accidentes es en intersecciones y luego Lote o Predio
- Los tipos de via con menos accidentes son via peatonal, paso a nivel, tunel y pontón
"""

result_cluster_2 = result[result['cluster'] == 2]
result_cluster_2

"""***Cluster 3:*** Es el grupo de barrios con mayor cantidad de accidentes viales.

*Caracteristicas:*

- Número de muertos: 21
- Número de choques: 3,774
- El tipo de via que presentó mayor cantidad de accidentes es en Tramo de via con 3,617
- El segundo tipo de via que presentó mayor cantidad de accidentes es en intersecciones
- Los tipos de via con menos accidentes son tunel y pontón
"""

result_cluster_3 = result[result['cluster'] == 3]
result_cluster_3

"""***Cluster 4:*** Es el segundo grupo de barrios con mayor cantidad de accidentes viales.

*Caracteristicas:*

- Número de muertos:21
- Número de choques: 2,655
- El tipo de via que presentó mayor cantidad de accidentes es en Tramo de via
- Los tipos de via con menos accidentes son via peatonal y pontón
"""

result_cluster_4 = result[result['cluster'] == 4]
result_cluster_4

"""***Cluster 5:*** Es el segundo grupo de barrios con menos cantidad de accidentes viales.

*Caracteristicas:*

- Número de muertos: 4
- Número de choques: 511
- El tipo de via que presentó mayor cantidad de accidentes es en Tramo de via e intersecciones
- Los tipos de via con menos accidentes son Paso Elevado, Paso Inferior, Paso a Nivel,	Pontón,	Puente,	Tramo de via Tunel y	Via peatonal.
"""

result_cluster_5 = result[result['cluster'] == 5]
result_cluster_5

"""##**Estrategias**
A continuación según vemos los barrios que pertenecen a cada grupo, para ver las caracteristicas de este y les asignamos estrategias, las cuales se podrian implementar para mejorar la situación de accidentalidad en el grupo.
"""

final=juntos2
final['cluster']=clustering.labels_

finalfinal=final
cluster=5
barriosEnCluster=finalfinal[final['cluster']==cluster]['barrio']
barriosEnCluster